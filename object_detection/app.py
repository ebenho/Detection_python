import streamlit as st
from ultralytics import YOLO
import tempfile
import cv2
import os

# Load YOLO model (COCO pre-trained)
model = YOLO("yolov8n.pt")  

st.set_page_config(page_title="Object Detection App", page_icon="üöÄ", layout="wide")
st.title("üöÄ Object Detection App")
st.write("Nh·∫≠n d·∫°ng **ng∆∞·ªùi, ƒë·ªì v·∫≠t, lo√†i v·∫≠t** trong ·∫£nh, video v√† webcam.")

# Sidebar ch·ªçn ch·∫ø ƒë·ªô
mode = st.sidebar.radio("Ch·ªçn ch·∫ø ƒë·ªô:", ["·∫¢nh", "Video", "Webcam"])

# ========== ·∫¢NH ==========
if mode == "·∫¢nh":
    uploaded_file = st.file_uploader("üì∑ Ch·ªçn ·∫£nh...", type=["jpg", "jpeg", "png"])
    if uploaded_file:
        # L∆∞u file t·∫°m
        temp_file = tempfile.NamedTemporaryFile(delete=False)
        temp_file.write(uploaded_file.read())
        temp_file.close()

        # Ch·∫°y detection
        results = model(temp_file.name)

        # Hi·ªÉn th·ªã k·∫øt qu·∫£
        for r in results:
            st.image(r.plot(), caption="K·∫øt qu·∫£ nh·∫≠n d·∫°ng", use_container_width=True)

        os.remove(temp_file.name)

# ========== VIDEO ==========
elif mode == "Video":
    uploaded_video = st.file_uploader("üé• Ch·ªçn video...", type=["mp4", "avi", "mov"])
    if uploaded_video:
        # L∆∞u file video t·∫°m
        temp_video = tempfile.NamedTemporaryFile(delete=False)
        temp_video.write(uploaded_video.read())
        temp_video.close()

        # Xu·∫•t video k·∫øt qu·∫£
        output_path = temp_video.name + "_out.mp4"
        results = model.predict(source=temp_video.name, save=True, project="outputs", name="video_results")

        # Hi·ªÉn th·ªã video
        st.video(f"outputs/video_results/{os.path.basename(temp_video.name)}")

# ========== WEBCAM ==========
elif mode == "Webcam":
    st.write("‚ö° Webcam realtime detection (ch·∫°y b·∫±ng OpenCV)")
    run = st.checkbox("B·∫≠t Webcam")
    if run:
        cap = cv2.VideoCapture(0)
        stframe = st.empty()
        while run:
            ret, frame = cap.read()
            if not ret:
                break
            results = model(frame)
            annotated_frame = results[0].plot()
            stframe.image(annotated_frame, channels="BGR", use_container_width=True)
        cap.release()
